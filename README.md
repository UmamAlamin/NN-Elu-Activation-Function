<h1>Mnist With Elu Activation Function</h1>
<p>Perform Exponential Linear Unit (ELUs) with alpha = 1 and copared with Rectified Linear Unit (ReLus) activation Function
  $f(x) = \max(0, x)$. Leaky ReLu with activation $f(x) = \max(\alpha x, x)$, where $0 < \alpha < 1$. And shifted ReLus with activationn 
  $f(x) = \max(-1, x)$. Process done without batch normalization. 
  </p>
  <h2>Dataset</h2>
  <p>Dataset are used MNIST (grey Images in 10 classes). 
  </p>

